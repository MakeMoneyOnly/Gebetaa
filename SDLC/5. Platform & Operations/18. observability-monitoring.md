# SKILL: observability-monitoring

## Purpose

Implement comprehensive observability including logging, monitoring, alerting, and tracing to understand system behavior, detect issues proactively, and debug problems quickly.

## When to Use

- When deploying to production
- When troubleshooting production issues
- When establishing SLIs/SLOs
- When building dashboards
- When setting up alerting

## Role Activated

**Senior Site Reliability Engineer + Observability Specialist**

You think like SREs at Google or Datadogâ€”observability is not optional, it's the foundation of reliability.

## Inputs (Variables)

```yaml
{application_stack}: Technologies in use
{criticality}: Business criticality level
{sli_targets}: Service level indicators
{existing_tools}: Current monitoring stack
{scale}: Traffic and data volume

Step-by-Step Process
Step 1: Structured Logging
typescriptDownloadCopy code// lib/logger.ts
import pino from 'pino';

const logger = pino({
  level: process.env.LOG_LEVEL || 'info',
  formatters: {
    level: (label) => ({ level: label }),
  },
  base: {
    env: process.env.NODE_ENV,
    service: process.env.SERVICE_NAME,
    version: process.env.APP_VERSION,
  },
  timestamp: () => `,"timestamp":"${new Date().toISOString()}"`,
  redact: {
    paths: ['password', 'token', 'authorization', 'cookie', '*.password', '*.token'],
    censor: '[REDACTED]',
  },
});

// Request context logger
export function createRequestLogger(requestId: string, userId?: string) {
  return logger.child({
    requestId,
    userId,
  });
}

// Structured log methods
export const log = {
  info: (message: string, data?: Record<string, unknown>) => 
    logger.info(data, message),
  
  warn: (message: string, data?: Record<string, unknown>) => 
    logger.warn(data, message),
  
  error: (message: string, error: Error, data?: Record<string, unknown>) => 
    logger.error({ ...data, error: { message: error.message, stack: error.stack } }, message),
  
  // Specific log types
  request: (method: string, path: string, statusCode: number, duration: number, data?: Record<string, unknown>) =>
    logger.info({ method, path, statusCode, duration, ...data }, 'HTTP Request'),
  
  db: (operation: string, table: string, duration: number, data?: Record<string, unknown>) =>
    logger.info({ operation, table, duration, ...data }, 'Database Query'),
  
  external: (service: string, operation: string, duration: number, success: boolean, data?: Record<string, unknown>) =>
    logger.info({ service, operation, duration, success, ...data }, 'External Service Call'),
};

// Express middleware
export const requestLogger = (req: Request, res: Response, next: NextFunction) => {
  const start = Date.now();
  const requestId = req.headers['x-request-id'] as string || crypto.randomUUID();
  
  req.logger = createRequestLogger(requestId, req.user?.id);
  res.setHeader('x-request-id', requestId);

  res.on('finish', () => {
    const duration = Date.now() - start;
    log.request(req.method, req.path, res.statusCode, duration, {
      requestId,
      userId: req.user?.id,
      userAgent: req.headers['user-agent'],
      ip: req.ip,
      query: req.query,
    });
  });

  next();
};
Step 2: Application Metrics
typescriptDownloadCopy code// lib/metrics.ts
import { collectDefaultMetrics, Counter, Histogram, Gauge, Registry } from 'prom-client';

const register = new Registry();

// Collect default Node.js metrics
collectDefaultMetrics({ register });

// Custom metrics
export const metrics = {
  // HTTP request metrics
  httpRequestDuration: new Histogram({
    name: 'http_request_duration_seconds',
    help: 'Duration of HTTP requests in seconds',
    labelNames: ['method', 'path', 'status_code'],
    buckets: [0.01, 0.05, 0.1, 0.5, 1, 2, 5],
    registers: [register],
  }),

  httpRequestTotal: new Counter({
    name: 'http_requests_total',
    help: 'Total number of HTTP requests',
    labelNames: ['method', 'path', 'status_code'],
    registers: [register],
  }),

  // Business metrics
  userSignups: new Counter({
    name: 'user_signups_total',
    help: 'Total number of user signups',
    labelNames: ['source'],
    registers: [register],
  }),

  activeUsers: new Gauge({
    name: 'active_users',
    help: 'Number of active users',
    registers: [register],
  }),

  // Database metrics
  dbQueryDuration: new Histogram({
    name: 'db_query_duration_seconds',
    help: 'Duration of database queries',
    labelNames: ['operation', 'table'],
    buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1],
    registers: [register],
  }),

  // External service metrics
  externalCallDuration: new Histogram({
    name: 'external_call_duration_seconds',
    help: 'Duration of external service calls',
    labelNames: ['service', 'operation', 'success'],
    buckets: [0.05, 0.1, 0.5, 1, 2, 5, 10],
    registers: [register],
  }),
};

// Metrics endpoint
export const metricsHandler = async (req: Request, res: Response) => {
  res.set('Content-Type', register.contentType);
  res.end(await register.metrics());
};

// Middleware to track HTTP metrics
export const metricsMiddleware = (req: Request, res: Response, next: NextFunction) => {
  const start = Date.now();
  
  res.on('finish', () => {
    const duration = (Date.now() - start) / 1000;
    const labels = {
      method: req.method,
      path: req.route?.path || req.path,
      status_code: res.statusCode.toString(),
    };
    
    metrics.httpRequestDuration.observe(labels, duration);
    metrics.httpRequestTotal.inc(labels);
  });
  
  next();
};
Step 3: Distributed Tracing
typescriptDownloadCopy code// lib/tracing.ts
import { NodeSDK } from '@opentelemetry/sdk-node';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';
import { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';
import { Resource } from '@opentelemetry/resources';
import { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';

const sdk = new NodeSDK({
  resource: new Resource({
    [SemanticResourceAttributes.SERVICE_NAME]: process.env.SERVICE_NAME,
    [SemanticResourceAttributes.SERVICE_VERSION]: process.env.APP_VERSION,
    [SemanticResourceAttributes.DEPLOYMENT_ENVIRONMENT]: process.env.NODE_ENV,
  }),
  traceExporter: new OTLPTraceExporter({
    url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT,
  }),
  instrumentations: [
    getNodeAutoInstrumentations({
      '@opentelemetry/instrumentation-fs': { enabled: false },
    }),
  ],
});

sdk.start();

// Manual span creation
import { trace, SpanStatusCode } from '@opentelemetry/api';

const tracer = trace.getTracer('app');

export async function withSpan<T>(
  name: string,
  fn: () => Promise<T>,
  attributes?: Record<string, string>
): Promise<T> {
  return tracer.startActiveSpan(name, async (span) => {
    try {
      if (attributes) {
        span.setAttributes(attributes);
      }
      const result = await fn();
      span.setStatus({ code: SpanStatusCode.OK });
      return result;
    } catch (error) {
      span.setStatus({
        code: SpanStatusCode.ERROR,
        message: error instanceof Error ? error.message : 'Unknown error',
      });
      throw error;
    } finally {
      span.end();
    }
  });
}

// Usage
const user = await withSpan(
  'fetchUser',
  () => db.users.findUnique({ where: { id } }),
  { userId: id }
);
Step 4: Error Tracking
typescriptDownloadCopy code// lib/error-tracking.ts
import * as Sentry from '@sentry/node';

Sentry.init({
  dsn: process.env.SENTRY_DSN,
  environment: process.env.NODE_ENV,
  release: process.env.APP_VERSION,
  
  // Performance monitoring
  tracesSampleRate: process.env.NODE_ENV === 'production' ? 0.1 : 1.0,
  
  // Error filtering
  beforeSend(event, hint) {
    // Don't send 404 errors
    if (event.exception?.values?.[0]?.type === 'NotFoundError') {
      return null;
    }
    return event;
  },
  
  // Add context
  integrations: [
    new Sentry.Integrations.Http({ tracing: true }),
    new Sentry.Integrations.Express({ app }),
    new Sentry.Integrations.Prisma({ client: prisma }),
  ],
});

// Express error handler
export const sentryErrorHandler = Sentry.Handlers.errorHandler({
  shouldHandleError(error) {
    // Only report 5xx errors
    return error.status === undefined || error.status >= 500;
  },
});

// Manual error capture with context
export function captureError(error: Error, context?: Record<string, unknown>) {
  Sentry.withScope((scope) => {
    if (context) {
      scope.setExtras(context);
    }
    Sentry.captureException(error);
  });
}
Step 5: Alerting Rules
yamlDownloadCopy code# prometheus/alerts.yml
groups:
  - name: application
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: |
          sum(rate(http_requests_total{status_code=~"5.."}[5m])) 
          / sum(rate(http_requests_total[5m])) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: High error rate detected
          description: Error rate is {{ \$value | humanizePercentage }} over the last 5 minutes

      # High latency
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High latency detected
          description: 95th percentile latency is {{ \$value | humanizeDuration }}

      # Database slow queries
      - alert: SlowDatabaseQueries
        expr: |
          histogram_quantile(0.95, sum(rate(db_query_duration_seconds_bucket[5m])) by (le, table)) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Slow database queries on {{ \$labels.table }}

      # Memory usage
      - alert: HighMemoryUsage
        expr: |
          process_resident_memory_bytes / 1024 / 1024 > 512
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: High memory usage
          description: Memory usage is {{ \$value | humanize }}MB

  - name: infrastructure
    rules:
      # Instance down
      - alert: InstanceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: Instance {{ \$labels.instance }} is down

      # Disk space
      - alert: LowDiskSpace
        expr: |
          (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: Low disk space
          description: Only {{ \$value | humanizePercentage }} disk space remaining
Step 6: Dashboard Configuration
jsonDownloadCopy code// grafana/dashboards/application.json
{
  "title": "Application Overview",
  "panels": [
    {
      "title": "Request Rate",
      "type": "graph",
      "targets": [
        {
          "expr": "sum(rate(http_requests_total[5m])) by (status_code)",
          "legendFormat": "{{ status_code }}"
        }
      ]
    },
    {
      "title": "Response Time (p95)",
      "type": "graph",
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))",
          "legendFormat": "p95"
        }
      ]
    },
    {
      "title": "Error Rate",
      "type": "singlestat",
      "targets": [
        {
          "expr": "sum(rate(http_requests_total{status_code=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m])) * 100"
        }
      ],
      "format": "percent",
      "thresholds": "1,5",
      "colors": ["green", "yellow", "red"]
    },
    {
      "title": "Active Users",
      "type": "singlestat",
      "targets": [
        {
          "expr": "active_users"
        }
      ]
    }
  ]
}
Step 7: SLI/SLO Definition
markdownDownloadCopy code## Service Level Objectives

### Availability SLO
**Target:** 99.9% uptime (43.8 minutes/month downtime)
**SLI:** `sum(rate(http_requests_total{status_code!~"5.."}[30d])) / sum(rate(http_requests_total[30d]))`

### Latency SLO
**Target:** 95% of requests < 500ms
**SLI:** `histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[30d])) by (le))`

### Error Budget
**Monthly budget:** 0.1% of requests (99.9% target)
**Current burn rate:** Tracked via Prometheus/Grafana

### Alerting Thresholds
| SLO | Warning | Critical |
|-----|---------|----------|
| Availability | < 99.95% | < 99.9% |
| Latency (p95) | > 400ms | > 500ms |
| Error Rate | > 0.5% | > 1% |
Outputs

1. Structured Logging - JSON logs with context
2. Application Metrics - Prometheus metrics
3. Distributed Tracing - OpenTelemetry setup
4. Error Tracking - Sentry integration
5. Alerting Rules - Prometheus alerting
6. Dashboards - Grafana configurations
7. SLI/SLO Definitions - Service objectives

Quality Gates
GateCriteriaStatusLoggingAll requests loggedMetricsCore metrics exposedTracingCritical paths tracedAlertingSLO alerts configuredDashboardsKey dashboards created
References & Best Practices
From Google SRE Book

* "Monitoring should answer: What's broken, and why?"
* The four golden signals: Latency, Traffic, Errors, Saturation.

From Julia Evans

* "Good logging is about asking: What questions will I need to answer?"